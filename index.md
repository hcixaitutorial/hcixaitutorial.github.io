**Date**: TBD

**Time**: TBD

**Location**: TBD

**Instructors**: <a href="http://qveraliao.com">Q. Vera Liao</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-moninder">Moninder Singh</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-zhangyun">Yunfeng Zhang</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-rachel">Rachel Bellamy</a>

**CHI Program** [link](https://chi2020.acm.org/authors/accepted-courses/#HotTopic22)

**Slides** [To be posted after CHI]



## Goal of the Course

We will address the following questions:

- **What is Explainable AI (XAI)?**

    What problems does XAI solve? What are the focuses of XAI work in various research communities (e.g., Machine Learning, HCI)? 

- **Why is XAI important?**

    What are the motivations for XAI? What applications or situations need XAI?

- **How to explain?**

    What are the state-of-the-art XAI techniques? How do they work? How to determine their suitability for different AI applications, users and contexts?

- **Where to start with XAI?**
 
    What tools are available for implementing XAI techniques? What guidelines are available for desigining XAI? Where to find relevant technical or educational resources?
    
- **What is next?**
    
    What are the important questions in XAI yet to be addressed? What are some promising future directions?
    
    
## Overview

The increasing adoption of AI, and in particular Machine Learning technologies that use opaque deep neural networks, has spurred great academic and public interest in Explainable AI (XAI). On the one hand, the research community is producing a rapidly growing collection of XAI techniques. On the other hand, to deliver satisfying user experiences with AI explanations requires user-centered approaches and interdisciplinary research to connect user needs and the technical advancement. In short, XAI is an area with growing needs and exciting opportunities for HCI research. 

This course is intended for HCI researchers and practitioners who are interested in developing and designing explanation features in AI systems, and those who want to understand the trends and core topics in XAI. In particular, we will draw on our experience working with industry design practitioners to discuss the opportunities and challenges to incorporate state-of-the-art XAI techniques in various AI applications [1]. 

The course will also introduce available toolkits that make it easy to create XAI, including [AIX 360](http://aix360.mybluemix.net), a comprehensive toolkit providing technical and educational resources for XAI [2]. Attendees will also have the opportunity to gain hands-on experience with implementing multiple styles of explanations with AIX 360 and provided code samples. 

**Outline**

- Introduction to XAI, its definition and motivations
- State-of-the-art XAI techniques, with real-world AI use cases such as [credit approval decision-support](http://aix360.mybluemix.net/data)
- Overview of [AIX 360](http://aix360.mybluemix.net) and other available resources for XAI
- Hands-on experience with implementing XAI techniques (optional)

## Intended audience and prerequisites

The intended audience are any CHI attendees who have already, or expect to engage in developing, designing and researching on the topic of XAI. The course does not require advanced knowledge in AI, data science or programming, though a basic understanding of machine learning concepts such as classification, training data, and features could be helpful. The course will include a 15-20 minutes hands-on practice with Python code samples provided. The course instructors will provide instructions to use the code samples, as well as introductory materials for machine learning beforehand for interested attendees. 

## Instructions for Attendees

Coming soon



## References

[1] Liao, Q. V., Gruen, D., & Miller, S. (2020). <a href="https://arxiv.org/abs/2001.02478"> Questioning the AI: Informing Design Practices for Explainable AI User Experiences</a>. CHI 2020

[2] Arya, V., Bellamy, R. K., Chen, P. Y., Dhurandhar, A., Hind, M., Hoffman, S. C., ... & Mourad, S. (2019). <a href="https://arxiv.org/abs/1909.03012"> One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques</a>. 
