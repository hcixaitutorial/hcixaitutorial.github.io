**Date**: Thursday, April 30, 2020

**Time**: 11:00 AM - 12:15 PM

**Location**: TBD

**Instructors**: Q. Vera Liao, Moninder Singh, Yunfeng Zhang, Rachel Bellamy

CHI Program [link](https://chi2020.acm.org/authors/accepted-courses/#HotTopic22)
Slides [To be posted after CHI]



## Goal of the Tutorial

We will address the following questions:

- **What is Explainable AI (XAI)?**

    What are the focuses of XAI work in various research communities (e.g., Machine Learning, HCI)? What problems does XAI solve?

- **Why is XAI important?**

    What are the motivations for XAI work? What applications or situations need XAI?

- **How to explain?**

    What are the state-of-the-art XAI techniques? How do they work? How to determine their suitability for different AI applications, users and contexts?

- **Where to start with developing XAI?**
 
    What tools are available for implementing state-of-the-art XAI techniques? Where to find relevant technical or educational resources?
    
- **What is next?**
    
    What are the important questions in XAI research yet to be addressed? What are some promising future directions?
    
    
## Overview

The increasing adoption of AI, and in particular Machine Learning technologies that use opaque deep neural networks, has spurred great academic and public interest in Explainable AI (XAI).  On the one hand, the research community is producing a rapidly growing collection of XAI techniques. On the other hand, to deliver satisfying user experiences with AI explanations requires user-centered approaches and interdisciplinary research to connect user needs and the technical advancement. In short, XAI is an area with growing needs and exciting opportunities for HCI research. 

This course is intended for HCI researchers and practitioners who are interested in developing and designing explanation features in AI systems, and those who want to understand the trends and core topics in XAI. We will particularly draw on our experience working with industry practitioners to discuss the opportunities and challenges to incporate state-of-the-art XAI techniques in various AI applications[1]. 

The course will also introduce available toolkits that make it easy to create XAI, including [AIX 360](http://aix360.mybluemix.net), a comprehensive toolkit providing technical and educational resources for XAI [2].Attendees will also the opportunity to gain hands-on experience with implementing multiple styles of explanations with AIX 360 and provided code samples. 

**Outline**

- Introduction to XAI, its definition and motivation
- State-of-the-art XAI techniques, with real-world AI use cases wuch as [credit approval decision-support](http://aix360.mybluemix.net/data)
- Overview of [AIX 360](http://aix360.mybluemix.net) and other available resources for XAI
- Hands-on experience with implementing XAI techniques (optional)

## Intended audience and prerequisites

The intended audience for this course are any CHI attendees who have already, or intend to engage in developing, designing and researching on the topic of XAI. The course does not require any advanced knowledge in AI, data science or programming, though a basic understanding of machine learning concepts such as classification, training data, and features could be helpful. The course will include a 15-20 minutes hands-on practice with Python code samples provided. The course instructors will provide instructions to use the code samples, as well as introductory materials for machine learning and Python programming beforehand for interested attendees. 

## Instructions for Attendees

Coming soon

## Instructors

**Q. Vera Liao**

Vera is a Research Staff Member in IBM Research AI. Her research interests are on human-AI interaction and intelligent user interfaces. Her current work focuses on explainable AI, uman centered machine learning and user-aware conversational agents.  Her work contributed to IBM's AI Explainability 360, an open-source toolkit providing educational and technical resources for AI explainability.  Recently, she organized several workshops and a panel on topics that connect the HCI and AI communities at  CHI, CSCW and IUI. She received her Ph.D. in Computer science from the University of Illinois at Urbana-Champaign.

**Moninder Singh**

Moninder is a Research Staff Member in the IBM Research AI organization at the IBM T. J.Watson Research Center. He received his Ph.D. in Computer and Information Science from the University of Pennsylvania in 1998. He is primarily interested in developing and deploying solutions for interesting analytics and decision support problems. His main research areas are machine learning and data mining, artificial intelligence, data privacy, information retrieval, probabilistic modeling and reasoning, and text mining. He has been actively working in issues of fairness and trust in AI, has contributed to the IBM's AI Fairness 360 and AI Explainability 360 open source toolkits, and has given several tutorials/talks and published papers on issues relating to trust in AI models.


**Yunfeng Zhang**

Yunfeng is a Research Staff Member at IBM Research AI. His research interests lie in the intersection between HCI and AI. His recent research projects involved creating conversational agents, modeling social interactions, and studying AI explainability, fairness, and trust. He contributed to the IBM's AI Fairness 360 and AI Explainability 360 open source toolkits, which are designed to help AI developers create intelligible and fair AI solutions. He received his Ph.D. in computer and information science from the University of Oregon in 2015.

**Rachel K.E. Bellamy**

Rachel is a Principal Research Scientist and Chair of the Computer Sciences Council at IBM T J Watson Research Center. In this role she heads a Council that manages a Research portfolio of exploratory science projects. Prior to this, she led an interdisciplinary team of human-computer interaction researchers, user experience designers and software engineers. That team most recently contributed to several IBM Research's Trusted AI projects, including the AI Fairness 360 and AI Explanability 360. Rachel received her doctorate in cognitive psychology from University of Cambridge, UK. She received a Bachelor of Science in psychology with mathematics and computer science from University of London. Before coming to IBM Research, she worked in Apple Computer's Advanced Technology Group researching software support for collaborative learning.


## References

[1] Liao, Q. V., Gruen, D., & Miller, S. (2020). <a href="https://arxiv.org/abs/2001.02478"> Questioning the AI: Informing Design Practices for Explainable AI User Experiences</a>. To appear in CHI 2020
[2] Arya, V., Bellamy, R. K., Chen, P. Y., Dhurandhar, A., Hind, M., Hoffman, S. C., ... & Mourad, S. (2019). <a href="https://arxiv.org/abs/1909.03012"> One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques</a>. 
