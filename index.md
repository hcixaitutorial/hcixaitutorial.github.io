**Date**: TBD

**Time**: Note this course will be taught twice at CHI 2021:

Monday May 10: CEST 17-19 / PDT 08-10 / EDT 11-13 / JST 00-02 (next day)

Wednesday May 12: CEST 17-19 / PDT 08-10 / EDT 11-13 / JST 00-02 (next day)


**Location**: TBD

**Instructors**: <a href="http://qveraliao.com">Q. Vera Liao</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-moninder">Moninder Singh</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-zhangyun">Yunfeng Zhang</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-rachel">Rachel Bellamy</a>

**CHI Program** TBD

**Slides** [To be posted after CHI]



## Goals of the Course

We will address the following questions:

- **What is Explainable AI (XAI)?**

    What problems does XAI solve? What are the focuses of XAI work in various research communities (e.g., Machine Learning, HCI)? 

- **Why is XAI important?**

    What are the motivations for XAI? What applications or situations need XAI?

- **How to explain?**

    What are the state-of-the-art XAI techniques? How to determine their suitability for different AI applications, users and contexts? How to design XAI user experiences?

- **Where to start with XAI?**
 
    What tools are available for implementing XAI? What guidelines are available for designing XAI? Where to find relevant resources?
    

    
    
## Overview

Artificial Intelligence (AI) technologies are increasingly used to make decisions and perform autonomous tasks in critical domains such as healthcare, finance, and employment. The needs to understand AI in order to improve, contest, develop appropriate trust and better interact with AI systems have spurred great academic and public interest in Explainable AI (XAI). On the one hand, the rapidly growing collection of XAI techniques allows diverse styles of explanations to be incorporated in AI systems. On the other hand, to deliver satisfying user experiences with AI explanations requires user-centered approaches and interdisciplinary research to connect user needs and technical advancement. In short, XAI is an area with growing needs and exciting opportunities for HCI research. 

This course is intended for HCI researchers and practitioners who are interested in developing and designing explanation features in AI systems, and those who want to understand the trends and core topics in the XAI literature. The course will introduce available toolkits that make it easy to create explanations for ML models, including <a href="http://aix360.mybluemix.net">AIX 360</a> [1], a comprehensive toolkit providing technical and educational resources on the topic such as introduction to XAI concepts, python code libraries, and tutorials.

We will also draw on our experience designing and studying XAI systemsn[3-6], as well as learning from industry design practitioners [2] to discuss opportunities and challenges to incorporate state-of-the-art XAI techniques in AI systems, including a "question-driven XAI design process"[7] we developed through our research.

**Outline**

- Overview of XAI
- Introduction to XAI methods and toolkits, including [AIX 360](http://aix360.mybluemix.net), with real-world AI use cases such as [credit approval decision-support](http://aix360.mybluemix.net/data)
- Introduction to resources for desigining XAI user experiences. 
- Hands-on experience with AIX 360 (optional)

## Intended audience and prerequisites

The intended audience for this course are any CHI attendees who have already, or intend to engage in developing, designing and researching on the topic of XAI. The course does not require advanced knowledge in AI, data science or programming, though a basic understanding of machine learning concepts such as classification, training data, and features could be helpful. 

The course will include 15-20 minutes hands-on practice with Python code samples provided. Interested attendees could further explore the code but programming is not required. The course instructors will provide instructions to use the code samples, as well as introductory materials for machine learning and Python programming beforehand for interested attendees.

## Instructions for Attendees

Coming soon



## References


[1] Arya, V., Bellamy, R. K., Chen, P. Y., Dhurandhar, A., Hind, M., Hoffman, S. C., ... & Mourad, S. (2019). <a href="https://arxiv.org/abs/1909.03012"> One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques</a>. 

[2] Liao, Q. V., Gruen, D., & Miller, S. (2020). <a href="https://arxiv.org/abs/2001.02478"> Questioning the AI: Informing Design Practices for Explainable AI User Experiences</a>. CHI 2020

[3] Dodge, J., Liao, Q. V., Zhang, Y., Bellamy, R. K., & Dugan, C (2019). <a href="https://arxiv.org/abs/1901.07694"> Explaining models: an empirical study of how explanations impact fairness judgmen</a>. IUI 2019

[4] Zhang, Y., Liao, Q. V., & Bellamy, R. K.  (2019). <a href="https://arxiv.org/abs/2001.02114"> ffect of confidence and explanation on accuracy and trust calibration in ai-assisted decision making. </a>. FAT* 2020

[5] Ghai, B., Liao, Q. V., Zhang, Y., Bellamy, R., & Mueller, K. (2021). <a href="https://arxiv.org/abs/2001.09219"> Explainable Active Learning (XAL) Toward AI Explanations as Interfaces for Machine Teachers</a>. CSCW 2021

[6] Ehsan, U., Liao, Q. V., Muller, M., Riedl, M. O., & Weisz, J. D. (2021). <a href="https://arxiv.org/abs/2101.04719"> Expanding Explainability: Towards Social Transparency in AI systems</a>. CHI 2021

[7] Liao, Q. V., PribiÄ‡, M., Han, J., Miller, S., & Sow, D. (2021). <a href="https://arxiv.org/abs/2101.04719"> Question-Driven Design Process for Explainable AI User Experiences</a>. Working Paper
