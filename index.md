**Date**: TBD

**Time**: TBD

**Location**: TBD

**Instructors**: <a href="http://qveraliao.com">Q. Vera Liao</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-moninder">Moninder Singh</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-zhangyun">Yunfeng Zhang</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-rachel">Rachel Bellamy</a>

**CHI Program** TBD

**Slides** [To be posted after CHI]



## Goal of the Course

We will address the following questions:

- **What is Explainable AI (XAI)?**

    What problems does XAI solve? What are the focuses of XAI work in various research communities (e.g., Machine Learning, HCI)? 

- **Why is XAI important?**

    What are the motivations for XAI? What applications or situations need XAI?

- **How to explain?**

    What are the state-of-the-art XAI techniques? How to determine their suitability for different AI applications, users and contexts? How to design XAI user experiences?

- **Where to start with XAI?**
 
    What tools are available for implementing XAI? What guidelines are available for designing XAI? Where to find relevant resources?
    
- **What is next?**
    
    What are the important questions in XAI yet to be addressed? What are some promising future directions?
    
    
## Overview

Artificial Intelligence (AI) technologies are increasingly used to make decisions and perform autonomous tasks in critical domains such as healthcare, finance, and employment. The needs to understand AI in order to improve, contest, develop appropriate trust and better interact with AI systems have spurred great academic and public interest in Explainable AI (XAI). On the one hand, the rapidly growing collection of XAI techniques allows diverse styles of explanations to be incorporated in AI systems. On the other hand, to deliver satisfying user experiences with AI explanations requires user-centered approaches and interdisciplinary research to connect user needs and technical advancement. In short, XAI is an area with growing needs and exciting opportunities for HCI research. 

This course is intended for HCI researchers and practitioners who are interested in developing and designing explanation features in AI systems, and those who want to understand the trends and core topics in the XAI literature. The course will introduce available toolkits that make it easy to create explanations for ML models, including AIX 360 [2], a comprehensive toolkit providing technical and educational resources on the topic such as introduction to XAI concepts, python code libraries, and tutorials.

We will also draw on our experience working with industry design practitioners to discuss opportunities and challenges to incorporate state-of-the-art XAI techniques in AI systems, including a "question-driven XAI design process" we developed in the practice [1].

**Outline**

- Introduction to XAI, its definition and motivations
- State-of-the-art XAI techniques, with real-world AI use cases such as [credit approval decision-support](http://aix360.mybluemix.net/data)
- Overview of [AIX 360](http://aix360.mybluemix.net) and other available resources for XAI
- Hands-on experience with implementing XAI techniques (optional)

## Intended audience and prerequisites

The intended audience are any CHI attendees who have already, or expect to engage in developing, designing and researching on the topic of XAI. The course does not require advanced knowledge in AI, data science or programming, though a basic understanding of machine learning concepts such as classification, training data, and features could be helpful. The course will include a 15-20 minutes hands-on practice with Python code samples provided. The course instructors will provide instructions to use the code samples, as well as introductory materials for machine learning beforehand for interested attendees. 

## Instructions for Attendees

Coming soon



## References

[1] Liao, Q. V., Gruen, D., & Miller, S. (2020). <a href="https://arxiv.org/abs/2001.02478"> Questioning the AI: Informing Design Practices for Explainable AI User Experiences</a>. CHI 2020

[2] Arya, V., Bellamy, R. K., Chen, P. Y., Dhurandhar, A., Hind, M., Hoffman, S. C., ... & Mourad, S. (2019). <a href="https://arxiv.org/abs/1909.03012"> One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques</a>. 
